## TFG: Optimizaci√≥n usando t√©cnicas de c√≥mputo de alto rendimiento aplicado a la IA
---
#### Estudiante: Daniel Pizarro Gallego (GII)

#### Dirigido por: Alberto N√∫√±ez Covarrubias
---
## √çndice

1. [MPI](#mpi)
2. [Aprendizaje por Refuerzo](aprendizaje-por-refuerzo)
3. [Programaci√≥n Evolutiva](programaci√≥n-evolutiva)
4. [Aprendizaje no Supervisado](aprendizaje-no-supervisado)
5. [Aprendizaje Supervisados](#aprendizaje-supervisado)

## Python
Es uno de los lenguajes m√°s populares para la IA debido a su sintaxis sencilla, amplia variedad de bibliotecas de IA (como TensorFlow, PyTorch, scikit-learn, etc.) y una gran comunidad de desarrolladores. Pero es un lenguaje bastante lento, lo que provoca que a la hora de ejecutar el c√≥digo tarde mucho tiempo en finalizar.

Voy a estar programando en python, y probando maneras de reducir el tiempo de ejecuci√≥n.

### Mejoras
- **No usar recursi√≥n**
- **Manejo de Interrupciones**
- **Divisi√≥n del Espacio de B√∫squeda:** Se puede reducir el tiempo de ejecucion de lineal a logaritmico, produciendo, potencias de 2 o 10, hilos.
- **C√°lculo ideal del N√∫mero de Workers:** Crear y gestionar demasiados hilos pueden generar un costo adicional, tanto en espacio como en tiempo.

---

## MPI
[MPI](https://mpi4py.readthedocs.io/en/stable/) es un est√°ndar para una biblioteca de paso de mensajes. El objetivo es comunicar procesos en ordenadores remotos.

Actualmente hay varias implementaciones: [Open MPI](http://www.open-mpi.org/), [MPICH](http://www.mpich.org/) , [MVAPICH](http://mvapich.cse.ohio-state.edu/), [IBM Platform MPI](http://www.ibm.com/systems/es/platformcomputing/products/mpi/), [Intel MPI](https://www.intel.com/content/www/us/en/developer/tools/oneapi/mpi-library.html)

MPI 2.0 tiene m√°s de 100 funciones. Se intercambia informaci√≥n usando paso de mensajes.

### La Ley de Amdahl (1967)
```
Acceleraci√≥n = 1/((1-p) + p/n)
```
**Memoria Compartida:** Lecturas en paralelo, escrituras con exclusi√≥n mutua (Mutex). **1 Nodo, n procesos.**

**Memoria Distribuida** Un proceso s√≥lo tiene acceso a su espacio de memoria. El proceso **X le env√≠a al proceso Y datos**. **n Nodos, n procesos.**

Taxonom√≠a de Flynn
- SISD (Single Instruction, Single Data stream) 
- SIMD (Single Instruction, Multiple Data streams) 
- MISD (Multiple Instruction, Single Data stream), Poco com√∫n 
- MIMD (Multiple Instruction, Multiple Data streams)

## Single Program Multiple Data (SPMD) 
**1 programa** ejecutado en paralelo. El mismo programa se **copia a todos los nodos**. Cada proceso tiene su propio su ID (Rank) 

![MPI](https://github.com/Danipiza/TFG/assets/98972125/9725df3b-60ba-4d4d-9726-0f5087f5c37b)

---

## Funciones 

### Funciones de Entorno
**Comunicador** agrupa los procesos en ‚Äúcomunicadores‚Äù. Los procesos que intercambian mensajes comparten comunicador ```comm=MPI.COMM_WORLD```

**Inicializar**. Establecer un entorno de MPI. Se invoca autom√°ticamente al importar mpi4py, se puede hacer manual si se desactiva la configuracion base.
```
mpi4py.rc.initialize = False
MPI.Init()
```

**Finalizar**. Terminar el procesamiento de MPI. Tiene que ser la ultima llamada MPI. Se invoca autom√°ticamente, se puede hacer manual si se desactiva la configuracion base.
```
mpi4py.rc.finalize = False
MPI.Finalize()
```

**Size**. Para comprobar el n√∫mero de procesos relacionados con el comunicador (tambi√©n cuenta el master). ```comm.Get_size()```

**Rank**. Para comprobar el ID (rank) del proceso asociado a un comunicador [0,(size-1)] ```comm.Get_rank()```

**Status**. Se utiliza para almacenar informaci√≥n sobre el mensaje recibido. Origen del mensaje, tag asociado, tama√±o del mensaje. Es un objeto Status que proporciona esta informaci√≥n.

**MPI.ANY_SOURCE**. Se utiliza en funciones de recibir mensajes, especifica que recibe el mensaje de cualquier destinatario.

**Tag**. Es una etiqueta asociada con el mensaje que se env√≠a. Es una forma de etiquetar los mensajes y es √∫til cuando est√°s implementando una comunicaci√≥n entre m√∫ltiples procesos y necesitas distinguir entre diferentes tipos de mensajes.


**Abortar**. Fuerza la finalizaci√≥n de todos los procesos MPI. ```MPI_Abort()```


### Funciones Punto a Punto
Proceso **emisor y receptor** del mensaje

**S√çNCRONA**: El proceso **emisor espera** a que se realice el env√≠o del mensaje. Comunicaci√≥n **bloqueante**

**Enviar (sinc)** Env√≠a un mensaje de forma s√≠ncrona. El mensaje _data_ lo envia un proceso y lo recibe el _dest_.
```comm.send(data, dest=, status=)```

**Recibir (sinc)** Recibe un mensaje de forma s√≠ncrona. El mensaje _data_ lo guarda en una variable el proceso que recibe desde _source_.
```data = comm.recv(source=, tag=, status=)```


**AS√çNCRONA**: El proceso emisor env√≠a el mensaje y **contin√∫a su ejecuci√≥n** sin asegurarse de que el proceso receptor haya solicitado el mensaje. Comunicaci√≥n **no bloqueante**

**Enviar (asinc)**. Env√≠a un mensaje de forma as√≠ncrona. Mensaje lo puede recibir un proceso mediante MPI_Recv o MPI_IRecv
```comm.Isend(data, dest=destino)```

**Recibir (asinc)**. Recibe un mensaje de forma as√≠ncrona
```
data = comm.Irecv(source=, tag=, status=)
data.Wait() # De esta forma espera a recibir el mensaje
```
 
![Master_Worker](https://github.com/Danipiza/TFG/assets/98972125/bb3bb7ab-b896-4638-a83b-ec256823baf5)

---

## Aprendizaje por Refuerzo
### Reinforcement Learning
Se basa en experiencias y simulaciones, con prueba y error, recibiendo recompensas con las acciones tomadas (tambi√©n pueden ser negativas). Nadie le dice al agente que hacer, toma las decisiones con diferentes estrategias. En la etapa de entrenamiento suele ser de forma aleatoria. Una vez tiene un feedback del entrenamiento se toma las decisiones maximizando las recompensas obtenidas en experiencias pasadas.

### Algoritmo Q-Learning:
Mezcla entre programaci√≥n din√°mica y Monte Carlo. R=Matriz de recompensas.
- R=Matriz de recompensas.
- Q=Matriz (Estados x Acciones). Que acci√≥n elegir en cada estado. (mayor valor)

Aprende el camino si es una buena acci√≥n, Back Propagation (Como en redes neuronales).

S=estado actual. A=acci√≥n tomada. S‚Äô=Estado siguiente. Ai=una acci√≥n.

```Q(S, A) = (1‚àíŒ±)*Q(S, A) + Œ±*(R(S, A) + Œ≥*maxi Q(S‚Äô, Ai))```

### Hiperparametros:

- Œ± (tasa de aprendizaje): 
Deber√≠a disminuir a medida que contin√∫a adquiriendo una base de conocimientos cada vez mayor.
- Œ≥ (factor de descuento): 
A medida que se acerca cada vez m√°s al valor l√≠mite, su preferencia por la recompensa a corto plazo deber√≠a aumentar, ya que no estar√° el tiempo suficiente para obtener la recompensa a largo plazo, lo que significa que su gamma deber√≠a disminuir.
- œµ:  Evita que la acci√≥n siga siempre la misma ruta.
A medida que desarrollamos nuestra estrategia, tenemos menos necesidad de exploraci√≥n y m√°s explotaci√≥n para obtener m√°s utilidad de nuestra pol√≠tica, por lo que en vez de utilizar un valor fijo, a medida que aumentan los ensayos, √©psilon deber√≠a disminuir. Al principio un √©psilon alto genera m√°s episodios de exploraci√≥n y al final un √©psilon bajo explota el conocimiento aprendido.





### Mejorar 
Se puede lograr con varias estrategias. El contexto del problema es muy importante.

### Paralelizaci√≥n del entorno: 
Entrenando un agente en un entorno complejo, se puede dividir el proceso de entrenamiento en varios workers/hilos.
- Ejecutar en varios ‚Äúworkers‚Äú el programa en la misma celda.
-	Ejecutar en varios ‚Äúworkers‚Äù el programa en diferentes celdas.
-	Ejecutar varios ‚Äúworkers‚Äù asignando secciones del mapa.
-	Recorrer la matriz Q e ir actualizando los valores

  
### Explotaci√≥n y evaluaci√≥n simult√°neas: 
Adem√°s de explorar el entorno, puedes utilizar hilos para realizar simult√°neamente la explotaci√≥n (es decir, tomar decisiones basadas en el conocimiento actual del agente) y la evaluaci√≥n (es decir, medir el desempe√±o del agente en el entorno). Esto puede acelerar el proceso de aprendizaje al permitir que el agente ajuste su estrategia m√°s r√°pidamente.

### Optimizaci√≥n de Hiperpar√°metros: 
alpha, beta y gamma son los hiperpar√°metros que se usan para almacenar las experiencias del agente. Con vairos workers/hilos con diferentes hiperpar√°metros as√≠ comprobando de forma paralela cual ser√≠a la mejor configuracion. EJ, con t√©cnicas como la **b√∫squeda aleatoria** u **optimizaci√≥n bayesiana distribuida** para encontrar la mejor configuraci√≥n de hiperpar√°metros para el modelo de aprendizaje por refuerzo.

### Implementaci√≥n Eficiente de Algoritmos: 
Hay algoritmos de aprendizaje por refuerzo, que se pueden paralelizar manera eficiente.

 [**A3C (Asynchronous Advantage Actor-Critic)**](https://www.activeloop.ai/resources/glossary/asynchronous-advantage-actor-critic-a-3-c/#:~:text=A3C%2C%20or%20Asynchronous%20Advantage%20Actor,form%20of%20rewards%20or%20penalties.)  visto en la asignatura IA1, b√°sicamente el agente recibe un feedback de la operaci√≥n que ha ejecutado, dando recompensas (si es negativa es una penalizaci√≥n).
 
 [**PPO (Proximal Policy Optimization)**](https://openai.com/research/openai-baselines-ppo)

---

## Programaci√≥n Evolutiva

La programaci√≥n evolutiva es una t√©cnica de optimizaci√≥n inspirada en la teor√≠a de la evoluci√≥n biol√≥gica. Se basa en el concepto de selecci√≥n natural y evoluci√≥n de las poblaciones para encontrar soluciones a problemas complejos.

Una poblaci√≥n est√° compuesta por individuos. Un individuo tiene un cromosoma, que tiene uno o varios genes, que a su vez cada gen tiene 1 o varios alelos. Los individuos se pueden representar: 
-	Binarios: Cada alelo es un bit. Los rangos de n√∫meros naturales en binario son de 2N, para codificar un 127 en binario se necesitan 27 bits. Y si queremos a√±adir n√∫meros reales con una cierta precisi√≥n este n√∫mero de bits aumenta.
-	Reales: N√∫meros reales, este es m√°s f√°cil de manejar.

El **fenotipo** (Decodificaci√≥n) de un individuo son los rasgos observables, es decir el valor num√©rico. 

El **genotipo** (Codificaci√≥n) es la composici√≥n gen√©tica de un individuo.



### Plantilla b√°sica de una Algoritmo Evolutivo
``` 
poblacion = iniciar_poblacion(tam_poblacion)
evaluar_poblacion(poblacion)
while(<<condici√≥n>>):
  seleccion = seleccionar_poblacion()
  # Reproducci√≥n
  cruzar_poblacion(seleccion, prob_cruce)
  mutar_poblacion(seleccion, prob_muta)
  3 Elegir que individuos pasan a la siguiente generacion
  eleccion_poblacion(poblacion, seleccionados)
  evaluar_poblacion()
```
![PEV](https://github.com/Danipiza/TFG/assets/98972125/6eeb6388-e177-4f5c-bd92-73631620d6c3)

- Inicializar Poblaci√≥n:
Para inicializar la poblaci√≥n, se pasa por par√°metro el tama√±o de la poblacion, y se inicializan todos los Individuos.

Los Individuos suelen ser un array de bits, por lo que se recorre el individuo, es decir, el cromosoma y con un numero random se genera cada alelo del cromosoma.

Cada individuo puede tener muchos bits, debido a un tama√±o de cromosoma elevado. Si tenemos muchos individuos y el tama√±o del cromosoma es muy grande se puede distribuir la carga
de trabajo entre varios workers/hilos para que generen una parte y envien al master lo generado.

- Evaluar Poblaci√≥n:
Para evaluar la poblaci√≥n, se reciben los individuos de la poblaci√≥n y se calcula su fitness, como de buenos son estos individuos para el problema a resolver.

La funcion de aptitud calcula el fitness, recibiendo el array de bits de un individuo. 

Dependiendo del problema, puede ser un calculo r√°pido con el fenotipo asociado al individuo, o un c√°lculo que requiera recorrer todo el array de bits. Se puede distribuir la carga de trabajo entre varios workers/hilos para calcular el fitness de una parte de la poblaci√≥n. (Como a la hora de inicializar)

- Seleccionar Poblaci√≥n:
Para seleccionar la poblacion, hay varios m√©todos, ruleta, torneo, estoc√°stico universal...

Se recibe el fitness de la poblaci√≥n y el m√©todo se encarga de elegir individuos, puede elegir a los mejores o elegir de manera justa.

Por ejemplo en el m√©todo de ruleta se selecciona un n√∫mero de individuos de la poblaci√≥n de manera aleatoria, pero teniendo en cuenta su adaptabilidad, por lo que los mejores individuos tienen m√°s probabilidades de ser escogidos. Se puede realizar un distribucion para que varios workers/hilos seleccionen una parte de la poblaci√≥n.

- Cruzar Poblaci√≥n:
Una vez seleccionados los individuos de la poblaci√≥n, con una probabilidad de cruce que se pasa por par√°metro se seleccionan 2 individuos aleatorios y se cruzan dependiendo de la probabilidad, si no se quedan como est√°n.

Hay varios m√©todos para el cruce, unos mas simples y otros m√°s complejos. Por ello se puede gestionar este proceso con workers/hilos que reciban una parte de los individuos seleccionados y devuelvan esa parte una vez realizada el cruce.

- Mutar Poblaci√≥n:
Cuando ya se han cruzado los elementos seleccionados estos pasan a una etapa de mutaci√≥n, en la cual con una probabilidad de mutaci√≥n, se recorre los alelos de los individuos seleccionados y cruzados para aplicar diversidad.

Al igual que con el cruce hay varios m√©todos, simples y complejos, que se pueden paralelizar con workers/hilos. 


---

## Aprendizaje no Supervisado

1. [Aglomerativo](#algoritmo-de-clustering-jer√°rquico-aglomerativo)
2. [KMedias](algoritmo-de-kmedias)

### Algoritmo de clustering jer√°rquico aglomerativo

```
-	FASE 1: Crear la matriz de distancias inicial D 
Es una matriz sim√©trica (basta con usar una de las matrices triangulares) - 
-	FASE 2: Agrupaci√≥n de Individuos 
    o	Partici√≥n inicial P0: Cada objeto es un cluster 
    o	Calcular la partici√≥n siguiente usando la matriz de distancias D
        ÔÇß	Elegir los dos clusters m√°s cercanos. Ser√°n la fila y la columna del m√≠nimo de la matriz D 
        ÔÇß	Agrupar los dos en un cluster. Eliminar de la matriz la fila y columna de los clusters agrupados. Generar la nueva matriz de distancias D. 
            ‚Ä¢	A√±adir una fila y una columna con el cluster nuevo 
            ‚Ä¢	Calcular la distancia del resto de clusters al cluster nuevo 
    o	Repetir paso 2 hasta tener s√≥lo un cluster con todos los individuos ‚Ä¢ Representar el dendograma (√°rbol de clasificaci√≥n) 
La complejidad con los enlaces simple y completo tienen un coste c√∫bico O(n3). Existen implementaciones m√°s eficientes (ùëõ2).
```

 
### Algoritmo de KMedias: 
**Algoritmos de clustering basados en particiones**

Se fija un valor k. 
1. Inicializar los k centros (o centroides) de los clusters de forma aleatoria.
- Generando puntos aleatorios en el espacio dimensional
- Seleccionando aleatoriamente individuos
2. Repite el siguiente proceso hasta que los centros no cambien.

**Fase de asignaci√≥n:** Para cada individuo se le asigna el cluster m√°s cercano. Requiere el uso de una distancia (normalmente eucl√≠dea, tambi√©n se puede usar manhattan o Chebychev)

**Actualiza el centro de los clusters.** Se calcula con la media de los individuos del Como los clusters se generan aleatoriamente, no siempre va a dar un buen resultado a la primera. Por lo que se pasa por par√°metro cuantas veces **se repite hasta encontrar el mejor**. Encontrar el **valor ideal para k**.


---

## Aprendizaje Supervisado
1. [KNN](#knn)
2. [Redes Neuronales](redes-neuronales)

### Validacion Cruzada
1. Los datos se dividen aleatoriamente en k subconjuntos iguales 
2. Se entrena el conjunto con (k-1) y se valida con el restante 
3. Se repite k veces el paso 2, cambiando el conjunto que se usa para validar 
4. Como medida de error final se suele presentar la media de las k medidas de error de validaci√≥n (aunque puede ser interesante comprobar que las k medidas sean similares)

![Validacion_Cruzada](https://github.com/Danipiza/TFG/assets/98972125/3f08fd2b-495e-48e2-846b-05c04d1a60cf)

### KNN
Es simple pero potente y se basa en la idea de que los puntos de datos similares tienden a agruparse en el espacio de caracter√≠sticas.
	1. Empezar con un dataset con categor√≠as conocidas.
	2. A√±adir un nuevo individuo con categor√≠a desconocida.
	3. Clasificar el individuo con los k vecinos m√°s cercanos.
Se puede aplicar a mapas de calor

No hay una forma de determinar el mejor valor para k, por lo que hay que probar con varias ejecuciones. valores peque√±os de k crea sonido. Valores grandes con pocos datos har√° que siempre sea la misma categor√≠a


### Redes Neuronales
![Esquema_red](https://github.com/Danipiza/TFG/assets/98972125/cf25aa2b-00e0-45e7-877f-d97f891f1ec4)
-	La primera columna es la capa de entrada de la variable. 
-	La √∫ltima columna es la capa de salida con todas las posibles salidas. 
-	Las columnas intermedias son las capas ocultas

Cada neurona hace una operaci√≥n simple. Suma los valores de todas las neuronas de la columna anterior. Estos valores multiplicados por un peso que determina la importancia de conexi√≥n entre las dos neuronas. Todas las neuronas conectadas tienen un peso que ir√°n cambiando durante el proceso de aprendizaje. Adem√°s, un valor bias puede ser a√±ado al valor calculado. No es un valor que viene de una neurona especifica y se escoge antes de la fase de aprendizaje. Puede ser √∫til para la red. Con el valor calculado se aplica a una funci√≥n de activaci√≥n, para obtener el valor final. Se suele usar para dar un valor entre [0-1].
Proceso de aprendizaje/entrenamiento:
Lo que queremos que haga la red neuronal, es que dado una entrada devuelva una salida. Al principio no va a ser as√≠, solo por suerte devuelve la salida correcta. Por esto se genera la etapa de aprendizaje, en la que cada entrada tiene asociada una etiqueta, para explicar que salida deber√≠a de haber adivinado.
-	Acierta: los par√°metros actuales se guardan, y se env√≠a la siguiente entrada
-	Falla: los pesos son cambiados. Se suele usar backpropagation

